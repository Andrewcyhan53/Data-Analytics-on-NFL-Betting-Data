# -*- coding: utf-8 -*-
"""Final_Project_Andrew_Cyhaniuk.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tP-cRQsrLHarSXlpDvAfroTTD1PdY_8q
"""

!pip install pyod            # normal install
!pip install --upgrade pyod  # or update if needed
!pip install mlxtend --upgrade

#Imports
import csv
import math
import random
import datetime
import numpy as np
from numpy import mean
from numpy import std
import pandas as pd
from pyod.models.iforest import IForest
from pyod.utils.example import visualize
from pyod.utils.utility import precision_n_scores
from sklearn.metrics import roc_auc_score
from sklearn import tree
from pyod.utils.data import evaluate_print
import matplotlib.pyplot as plt
from sklearn import metrics
from sklearn.tree import export_text
from matplotlib import colors
from matplotlib.ticker import PercentFormatter
from google.colab import drive
from sklearn import preprocessing
from sklearn import utils
from sklearn.datasets import make_classification
from sklearn.metrics import silhouette_score
from sklearn.cluster import KMeans
from sklearn.cluster import DBSCAN
from sklearn.decomposition import PCA
from scipy.cluster.hierarchy import linkage,dendrogram,cut_tree
from yellowbrick.cluster import SilhouetteVisualizer
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
#from mlxtend.frequent_patterns import apriori, fpgrowth, fpmax, association_rules

"""Project Outline:


1.  Process Data
2.  Run decision classification method the odds based on wins or losses to try and determine important features
3. Run a Random Forest and Neural Network classifier to see if it yields better results
5. Use anomaly detection to try and find bets that hit big or loose and if they have any similarities in attributes
6. Graph results over time and other performance metrics
7. Test the results on the NFL games over the weeks before the project is due (There is 2 weeks so 32 games).




"""

#Main method and Method Calls to set parameters
if __name__ == "__main__":
  data,newData,trainData,trainVals,testData,testVals=getData()
  #Turn on to generate decision tree parameters trainData, trainVals, testVals, testData, TREE_CRITERION, MAX_HEIGHT, Height
  decTree(trainData,trainVals,testVals,testData,'entropy',7,1)
  #Turn on to generate clusters Data, Kmin, Kmax
  cluster(newData,2,10)
  #Turn on for Random Forest
  rfClassify(trainData,trainVals,testData,testVals,minSplits,maxSplits)
  #Turn on forNeural Network
  nnClassify(trainData,trainVals,testData,testVals)
  #Turn on for anomoly detection Implement using Dbscan for the esp = .8 for Y train & Test
  anomDect(trainData,testData,trainVals,testVals,0.08)

#Data importation and preprocessing
def getData():
  drive.mount('/content/drive')
  input_file_name = '/content/drive/MyDrive/Cyhaniuk_Andrew_CSC_373/Final Project/footballspread.csv'
  with open(input_file_name,"r") as input_file_ptr:
    data = pd.read_csv(input_file_ptr)
    data['schedule_season']=data['schedule_season'].astype(int)
    data=data.drop(columns='weather_humidity')
    data=data.drop(columns='weather_detail')
    data=data.drop(columns='schedule_date')
    #The gambaling data such as spread and over under line was added in 1979
    data=data.loc[data['schedule_season']>=1979]
    data=data.dropna()
    data=data.reset_index()
    data=data.drop(columns='index')
    teams=data['team_home'].unique()
    teamsa=data['team_away'].unique()
    for x in teamsa:
      if x not in teams:
        teams=np.append(teams,x)
    stadiums=data['stadium'].unique()
    favorite=data['team_favorite_id'].unique()
    sw=data['schedule_week'].unique()
    sw=sorted(sw)
    teams= sorted(teams)
    stadiums=sorted(stadiums)
    favorite=sorted(favorite)
    #for loop for processing
    sy=[]
    sched=[]
    th=[]
    ta=[]
    stad=[]
    sn=[]
    sp=[]
    tf=[]
    oul=[]
    for x in range(len(data)):
      sy.append(int(data.iloc[x]['schedule_season'])-1978)
      th.append(teams.index(data.iloc[x]['team_home']))
      sched.append(sw.index(data.iloc[x]['schedule_week']))
      ta.append(teams.index(data.iloc[x]['team_away']))
      stad.append(stadiums.index(data.iloc[x]['stadium']))
      tf.append(favorite.index(data.iloc[x]['team_favorite_id']))
      if data.iloc[x]['stadium_neutral']==False:
        sn.append(0)
      else:
        sn.append(1)
      if data.iloc[x]['schedule_playoff']==False:
        sp.append(0)
      else:
        sp.append(1)
      if data.iloc[x]['over_under_line']==' ':
        oul.append('0')
      else:
        oul.append(data.iloc[x]['over_under_line'])
    data['over_under_line']=oul
    data['schedule_season']=sy
    data['schedule_week']=sched
    data['team_home']=th
    data['team_away']=ta
    data['stadium']=stad
    data['stadium_neutral']=sn
    data['schedule_playoff']=sp
    data=data.drop(columns='team_favorite_id')
    data=data.dropna()
    newData=pd.DataFrame()
    datacol=data.columns
    for col in range(data.shape[1]):
      column=data.iloc[:,col]
      column=column.astype(float)
      newData.loc[:,datacol[col]]=(scale_data(column))
    #Using a bootstrapped sample to have a varied training and test set
    trainNum=int(.75*len(newData))
    #Calaulate bootstrap probability of repeating
    testNum=int(.25*len(newData))
    trainData=newData.sample(n=trainNum, replace=True)
    testData=newData.sample(n=testNum,replace=True)
    trainVals=[]
    i =0
    for l in range(len(trainData)):
      if trainData.iloc[l]['score_home']+trainData.iloc[l]['score_away']>trainData.iloc[l]['over_under_line']:
        trainVals.append(1)
      else:
        trainVals.append(0)
    trainData=trainData.drop(columns='score_home')
    trainData=trainData.drop(columns='score_away')
    testVals=[testNum]
    for l in range(len(testData)-1):
      if testData.iloc[l]['score_home']+testData.iloc[l]['score_away']>testData.iloc[l]['over_under_line']:
        testVals.append(1)
      else:
        testVals.append(0)
    testData=testData.drop(columns='score_home')
    testData=testData.drop(columns='score_away')
    return(data,newData,trainData,trainVals,testData,testVals)

#Calculate Statistics of prevelance, accuracy,precesion, specificity, and sensitivity,PPV
def calculate_stats(pred, act,OUL):
    OUL=OUL.reset_index()
    OUL=OUL.drop(columns='index')
    b=0
    totalCur=0
    FalP=0
    FalN=0
    CurP=0
    CurN=0
    for l in pred:
      if(l>OUL.iloc[b]['over_under_line']):
        totalCur+=1
        if(act[b]==1):
           CurP+=1
        else:
          FalP+=1
      else:
        if(act[b]==0):
           CurN+=1
        else:
           FalN+=1
      b+=1
    prevelance=(CurP+FalN)/b
    precesion=CurP/(CurP+FalP)
    sensitivity=CurP/(CurP+FalN)
    specificity=CurN/(FalP+CurN)
    PPV=CurP/(CurP+FalP)
    accuracy=(totalCur/b)
    return prevelance, accuracy,precesion, specificity, sensitivity,PPV

#Scale Data:
def scale_data(data):
    max=data.max()
    min=data.min()
    hold_val=[]
    for x in data:
     hold= x
     hold_val.append((hold-min)/(max-min))
    return hold_val

'schedule_date'
def scaleDate(date):
    hold= pd.to_datetime(date, infer_datetime_format=True)
    day_of_month_df1 = hold.dt.day
    return day_of_month_df1

#Decision tree
def decTree(trainData,trainVals,testVals,testData,TREE_CRITERION,MAX_HEIGHT,Height):
#Set parameters for the decision tree package
  #Set parameters for plotting the decision tree
    GRAPH_HEIGHT   = 10
    GRAPH_WIDTH    = 20
    predicted=[]
  #Open an output file to print performance metrics as some decision tree
    output_file_name = '/content/drive/MyDrive/Cyhaniuk_Andrew_CSC_373/Final Project/DecTree_Out.dat'
    with open(output_file_name,"w") as OP_Ptr:
      writer=csv.writer(OP_Ptr)
      writer.writerow(["Prevalance","Accuracy", "Sensitivity","Specificity","Precision","PPV"])
      #Loop on Height
      while(Height<MAX_HEIGHT):
    #Build and use the decision tree package. You need to fill in arguments.
        clf = tree.DecisionTreeClassifier(criterion=TREE_CRITERION,splitter="best",max_depth=Height)
        clf = clf.fit(trainData,trainVals)
        pred = clf.predict(testData)
        predicted.append(pred)
        prev, accur,  sens, spec, prec, ppv = calculate_stats(pred,testVals,testData['over_under_line'])
        print("The max height of the tree is", Height)
        print("{:14s} {:.3f}".format("Prevalence:", prev))
        print("{:14s} {:.3f}".format("Accuracy:", accur))
        print("{:14s} {:.3f}".format("Sensitivity:", sens))
        print("{:14s} {:.3f}".format("Specificity:", spec))
        print("{:14s} {:.3f}".format("Precision:", prec))
        print("{:14s} {:.3f}".format("PPV:", ppv))
        print("\n")
        writer.writerow([prev,accur,sens,spec,prec,ppv])
        plt.figure(figsize=(GRAPH_WIDTH, GRAPH_HEIGHT))
        tree.plot_tree(clf)
        Height+=1
      print("END Of Decesion Tree Algorithm")
      return predicted

#Clustering
def cluster(df,kmin,kmax):
  silhouette=[]
  for u in range(kmin,kmax):
    cent,intraClus,labelsK=Kmeans_Package(df,u)
    silhouette.append(round(silhouette_score(df,labelsK),3))
    kmeanTot=[]
    plotClus(u)
    for x in range(len(cent)):
     sumTat=0
     print("The centeriods for the KMeans package is number ",x, " is " ,round(cent[x],3),"\n")
     for b in range(len(labelsK)):
       if(labelsK[b]==x):
         sumTat= sumTat+1
       kmeanTot.append(sumTat)
    print("The intra cluster distance for the KMeans package  is ", round(intraClus,3),"for k=",x+1,"\n")
  for e in range(len(silhouette)):
    print("The silhouette score for k = ",e+1, " is ",round(silhouette[e],3),"\n")
  print("Sillouette score over k for the package clustering")
  plt.plot(silhouette)
  plt.show()
  for x in np.arange(.30, 1, 0.1):
    lablesD=DBSCAN_Pack(df,x,20)
def Kmeans_Package(data,k):
  kmeans=KMeans(n_clusters=k).fit(data)
  return kmeans.cluster_centers_,kmeans.inertia_,kmeans.labels_
def DBSCAN_Pack(data,ceps,minSamp):
  db=DBSCAN(eps=ceps,min_samples=minSamp).fit(data)
  labels = db.labels_

  # Number of clusters in labels, ignoring noise if present.
  n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
  n_noise_ = list(labels).count(-1)
  print("Estimated number of clusters: %d" % n_clusters_)
  print("Estimated number of noise points: %d" % n_noise_)
  print("Silhouette Coefficient: %0.3f" % metrics.silhouette_score(data,labels))
  return db.labels_

#Plot K-Means
def plotClus(k):
  reduced_data = PCA(n_components=2).fit_transform(data)
  kmeans = KMeans(init="k-means++", n_clusters=k, n_init=4)
  kmeans.fit(reduced_data)

  # Step size of the mesh. Decrease to increase the quality of the VQ.
  h = 0.02  # point in the mesh [x_min, x_max]x[y_min, y_max].

  # Plot the decision boundary. For that, we will assign a color to each
  x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1
  y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1
  xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

  # Obtain labels for each point in mesh. Use last trained model.
  Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])

  # Put the result into a color plot
  Z = Z.reshape(xx.shape)
  plt.figure(1)
  plt.clf()
  plt.imshow(
      Z,
      interpolation="nearest",
      extent=(xx.min(), xx.max(), yy.min(), yy.max()),
      cmap=plt.cm.Paired,
      aspect="auto",
      origin="lower",
  )

  plt.plot(reduced_data[:, 0], reduced_data[:, 1], "k.", markersize=2)
  # Plot the centroids as a white X
  centroids = kmeans.cluster_centers_
  plt.scatter(
      centroids[:, 0],
      centroids[:, 1],
      marker="x",
      s=169,
      linewidths=3,
      color="w",
      zorder=10,
  )
  plt.title(
      "K-means clustering on the digits dataset (PCA-reduced data)\n"
      "Centroids are marked with white cross"
  )
  plt.xlim(x_min, x_max)
  plt.ylim(y_min, y_max)
  plt.xticks(())
  plt.yticks(())
  plt.show()

# MLP Neural Network Classifier
def nnClassify(xtrain,ytrain,xtest,ytest):
  accuracy=[]
  for q in range(1,10):
   clf = MLPClassifier(solver='lbfgs', alpha=1e-5, random_state=1,max_iter=10000, hidden_layer_sizes= (q,q))
   clf.fit(xtrain, ytrain)
   acc=round(clf.score(xtest, ytest), 4)
   accuracy.append(acc)
   print("Score for hidden layers equal a ",q," by ",q,"matrix is:", acc)
  plt.plot(accuracy)
  plt.show()

#Random Forest Classifier
def rfClassify(xtrain,ytrain,xtest,ytest,minSplits,maxSplits):
  model = RandomForestClassifier(bootstrap=True)
  for i in range (minSplits,maxSplits):
    cv = RepeatedStratifiedKFold(n_splits=i, n_repeats=3, random_state=1)
    n_scores = cross_val_score(model, xtrain, ytrain, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')
    print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))
    print(n_scores)
    plt.plot(n_scores)
  plt.show()

#Anomaly detection
def anomDect(X_train,X_test,y_train,y_test,contamination):
  #Isolation Forest implementation
    clf_name = 'IForest'
    clf = IForest()
    clf.fit(X_train)


    y_train_pred = clf.labels_
    y_train_scores = clf.decision_scores_


    y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)
    y_test_scores = clf.decision_function(X_test)  # outlier scores

    # evaluate and print the results
    print(y_test_scores.mean())
    plt.plot(y_test_scores)
    plt.show()
    feature_importance = clf.feature_importances_
    print("Feature importance", feature_importance)